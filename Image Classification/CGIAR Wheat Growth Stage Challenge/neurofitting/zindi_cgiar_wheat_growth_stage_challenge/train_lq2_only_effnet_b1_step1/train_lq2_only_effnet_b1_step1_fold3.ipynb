{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_lq2_only_effnet_b1_step1_fold3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPP99bHk2oks4m2t2tEDEA8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"2c1b47a147984440b4ecb4778a317fd0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d73c0def9fe1481884248b3e047bb9f0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_5ef7833ca01e453898183e37c2b43539","IPY_MODEL_fefb9fc0bd1f4458b6ac2c296c88f8bd"]}},"d73c0def9fe1481884248b3e047bb9f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5ef7833ca01e453898183e37c2b43539":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9a6bc18b68f84b8a80d5822b88cda22c","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":31519111,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":31519111,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_58973bdc647643148e9aa0289d8cdf99"}},"fefb9fc0bd1f4458b6ac2c296c88f8bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_5ca6e6bd3f5b46e792e17608d15961d2","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 30.1M/30.1M [00:01&lt;00:00, 21.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c62eb5e653b24896812a0382e126642e"}},"9a6bc18b68f84b8a80d5822b88cda22c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"58973bdc647643148e9aa0289d8cdf99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5ca6e6bd3f5b46e792e17608d15961d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c62eb5e653b24896812a0382e126642e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"XL7J2ffWNoPQ"},"source":["# This colab notebook must be run on a **P100** GPU instance otherwise it will crash. Use the Cell-1 to ensure that it has a **P100** GPU instance "]},{"cell_type":"markdown","metadata":{"id":"LpU4e-qkNttp"},"source":["Cell-1: Ensure the required gpu instance (P100)"]},{"cell_type":"code","metadata":{"id":"SPfTSIZeNgFz","executionInfo":{"status":"ok","timestamp":1602058748162,"user_tz":-360,"elapsed":1551,"user":{"displayName":"Mr clb","photoUrl":"","userId":"05491251720306568104"}},"outputId":"0d3d441d-8f7f-46ff-fc10-c769d3bc2828","colab":{"base_uri":"https://localhost:8080/"}},"source":["#no.of sockets i.e available slots for physical processors\n","!lscpu | grep 'Socket(s):'\n","#no.of cores each processor is having \n","!lscpu | grep 'Core(s) per socket:'\n","#no.of threads each core is having\n","!lscpu | grep 'Thread(s) per core'\n","#GPU count and name\n","!nvidia-smi -L\n","#use this command to see GPU activity while doing Deep Learning tasks, for this command 'nvidia-smi' and for above one to work, go to 'Runtime > change runtime type > Hardware Accelerator > GPU'\n","!nvidia-smi"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Socket(s):           1\n","Core(s) per socket:  1\n","Thread(s) per core:  2\n","GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-e754b97b-d438-6a96-7c88-b140c0ab1ceb)\n","Wed Oct  7 08:21:59 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   34C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cgEg3UdIOBFw"},"source":["Cell-2: Add Google Drive"]},{"cell_type":"code","metadata":{"id":"5WEHfOvOOBxW","executionInfo":{"status":"ok","timestamp":1602058770971,"user_tz":-360,"elapsed":19285,"user":{"displayName":"Mr clb","photoUrl":"","userId":"05491251720306568104"}},"outputId":"79da70fe-d409-4d10-a892-a4bad64155eb","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HmESAHB6ODmO"},"source":["Cell-3: Install Required Dependencies"]},{"cell_type":"code","metadata":{"id":"qd9kcFgyOGAP","executionInfo":{"status":"ok","timestamp":1602058785336,"user_tz":-360,"elapsed":12278,"user":{"displayName":"Mr clb","photoUrl":"","userId":"05491251720306568104"}},"outputId":"24f67ee8-c89b-4be7-b74d-2dc41cf344b7","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install efficientnet_pytorch==0.7.0 \n","!pip install albumentations==0.4.5\n","!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch\\_stable.html -q\\"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting efficientnet_pytorch==0.7.0\n","  Downloading https://files.pythonhosted.org/packages/4e/83/f9c5f44060f996279e474185ebcbd8dbd91179593bffb9abe3afa55d085b/efficientnet_pytorch-0.7.0.tar.gz\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from efficientnet_pytorch==0.7.0) (1.6.0+cu101)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch==0.7.0) (1.18.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->efficientnet_pytorch==0.7.0) (0.16.0)\n","Building wheels for collected packages: efficientnet-pytorch\n","  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-cp36-none-any.whl size=16031 sha256=3023528227dbd64561a47d427395b2e7a969dc5248cbd7265fafca0e07a25db8\n","  Stored in directory: /root/.cache/pip/wheels/e9/c6/e1/7a808b26406239712cfce4b5ceeb67d9513ae32aa4b31445c6\n","Successfully built efficientnet-pytorch\n","Installing collected packages: efficientnet-pytorch\n","Successfully installed efficientnet-pytorch-0.7.0\n","Collecting albumentations==0.4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/40/a343ecacc7e22fe52ab9a16b84dc6165ba05ee17e3729adeb3e2ffa2b37b/albumentations-0.4.5.tar.gz (116kB)\n","\u001b[K     |████████████████████████████████| 122kB 9.6MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.18.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (1.4.1)\n","Collecting imgaug<0.2.7,>=0.2.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n","\u001b[K     |████████████████████████████████| 634kB 16.7MB/s \n","\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (3.13)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from albumentations==0.4.5) (4.1.2.30)\n","Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.16.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.15.0)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.1.1)\n","Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (7.0.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (3.2.2)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.5)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.4.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (1.2.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (2.8.1)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations==0.4.5) (4.4.2)\n","Building wheels for collected packages: albumentations, imgaug\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-0.4.5-cp36-none-any.whl size=64378 sha256=9c679cdaa9c2f402bfa53f74c09fce67246f08217f3a687c4d6dd7c444199c64\n","  Stored in directory: /root/.cache/pip/wheels/f0/a0/61/e50f93165a5ec7e7f5d65064e513239505bc4c06d2289557d3\n","  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for imgaug: filename=imgaug-0.2.6-cp36-none-any.whl size=654021 sha256=83f7ab28981fb1568e4417cde6cca0f1cd8bfa95f9b5723f0fba2e227c029aa5\n","  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n","Successfully built albumentations imgaug\n","Installing collected packages: imgaug, albumentations\n","  Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","  Found existing installation: albumentations 0.1.12\n","    Uninstalling albumentations-0.1.12:\n","      Successfully uninstalled albumentations-0.1.12\n","Successfully installed albumentations-0.4.5 imgaug-0.2.6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"axobq88eOJNg"},"source":["Cell-4: Run this cell to generate current fold weight ( Estimated Time for training this fold is around 1 hour 50 minutes )"]},{"cell_type":"code","metadata":{"id":"V-qSEALoOLvk","outputId":"1c9c938b-56cb-4e5a-bba5-ef36b88a5262","colab":{"base_uri":"https://localhost:8080/","height":468,"referenced_widgets":["2c1b47a147984440b4ecb4778a317fd0","d73c0def9fe1481884248b3e047bb9f0","5ef7833ca01e453898183e37c2b43539","fefb9fc0bd1f4458b6ac2c296c88f8bd","9a6bc18b68f84b8a80d5822b88cda22c","58973bdc647643148e9aa0289d8cdf99","5ca6e6bd3f5b46e792e17608d15961d2","c62eb5e653b24896812a0382e126642e"]}},"source":["import sys\n","sys.path.insert(0, \"/content/gdrive/My Drive/zindi_cgiar_wheat_growth_stage_challenge/src_lq2\")\n","\n","from dataset import *\n","from model import *\n","from trainer import *\n","from utils import *\n","\n","import numpy as np\n","from sklearn.model_selection import StratifiedKFold\n","from torch.utils.data import DataLoader\n","\n","config = {\n","    'n_folds': 5,\n","    'random_seed': 5400,\n","    'run_fold': 3,\n","    'model_name': 'efficientnet-b1', \n","    'global_dim': 1280,\n","    'batch_size': 92,\n","    'n_core': 2,\n","    'weight_saving_path': '/content/gdrive/My Drive/zindi_cgiar_wheat_growth_stage_challenge/train_lq2_only_effnet_b1_step1/weights/',\n","    'resume_checkpoint_path': None,\n","    'lr': 0.01,\n","    'total_epochs': 100,\n","    }\n","\n","\n","if __name__ == '__main__':\n","    set_random_state(config['random_seed']) \n","    \n","    imgs = np.load('/content/gdrive/My Drive/zindi_cgiar_wheat_growth_stage_challenge/zindi_npy_data/train_imgs.npy')\n","    labels = np.load('/content/gdrive/My Drive/zindi_cgiar_wheat_growth_stage_challenge/zindi_npy_data/train_labels.npy')\n","    labels_quality = np.load('/content/gdrive/My Drive/zindi_cgiar_wheat_growth_stage_challenge/zindi_npy_data/train_labels_quality.npy')\n","\n","    imgs = imgs[labels_quality == 2]\n","    labels = labels[labels_quality == 2]\n","    labels = labels - 1 \n","\n","    skf = StratifiedKFold(n_splits=config['n_folds'], shuffle=True, random_state=config['random_seed'])    \n","    for fold_number, (train_index, val_index) in enumerate(skf.split(X=imgs, y=labels)):\n","        if fold_number != config['run_fold']:\n","            continue\n","                        \n","        train_dataset = ZCDataset(\n","                            imgs[train_index],\n","                            labels[train_index],\n","                            transform=get_train_transforms(),\n","                            test=False,\n","                            )               \n","        train_loader = DataLoader(\n","                            train_dataset, \n","                            batch_size=config['batch_size'], \n","                            shuffle=True, \n","                            num_workers=config['n_core'],\n","                            drop_last=True,\n","                            pin_memory=True,        \n","                            )\n","\n","        val_dataset = ZCDataset(\n","                            imgs[val_index],\n","                            labels[val_index],\n","                            transform=get_val_transforms(),                          \n","                            test=True,\n","                            )        \n","        val_loader = DataLoader(\n","                            val_dataset, \n","                            batch_size=config['batch_size'], \n","                            shuffle=False, \n","                            num_workers=config['n_core'],\n","                            pin_memory=True,\n","                            )\n","        \n","        del imgs, labels\n","\n","        model = CNN_Model(config['model_name'], config['global_dim'])        \n","        \n","        args = { \n","                'model': model,\n","                'Loaders': [train_loader,val_loader],\n","                'metrics': {'Loss':AverageMeter, 'f1_score':PrintMeter, 'rmse':PrintMeter},                       \n","                'checkpoint_saving_path': config['weight_saving_path'],\n","                'resume_train_from_checkpoint': False,\n","                'resume_checkpoint_path': config['resume_checkpoint_path'],\n","                'lr': config['lr'],\n","                'fold': fold_number,\n","                'epochsTorun': config['total_epochs'],\n","                'batch_size': config['batch_size'],\n","                'test_run_for_error': False,\n","                'problem_name': 'zindi_cigar',\n","                }         \n","        Trainer = ModelTrainer(**args)\n","        Trainer.fit()        "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://github.com/lukemelas/EfficientNet-PyTorch/releases/download/1.0/efficientnet-b1-f1951068.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet-b1-f1951068.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c1b47a147984440b4ecb4778a317fd0","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=31519111.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Loaded pretrained weights for efficientnet-b1\n"],"name":"stdout"},{"output_type":"stream","text":["(Train) Fold 3 Epoch 1/100:   0%|          | 0/24 [00:00<?, ? batches/s]/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py:749: UserWarning: Image compression augmentation is most effective with uint8 inputs, float32 is used as input.\n","  UserWarning,\n","/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py:749: UserWarning: Image compression augmentation is most effective with uint8 inputs, float32 is used as input.\n","  UserWarning,\n","(Train) Fold 3 Epoch 1/100:   4%|▍         | 1/24 [00:01<00:40,  1.78s/ batches]/content/gdrive/My Drive/zindi_cgiar_wheat_growth_stage_challenge/src_lq2/optimizer.py:139: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n","  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n","(Train) Fold 3 Epoch 1/100: : 25 batches [00:44,  1.78s/ batches, TrainLoss=3.1356, Trainf1_score=0.0000, Trainrmse=0.0000]\n","(Valid) Fold 3 Epoch 1/100:   0%|          | 0/7 [00:00<?, ? batches/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["(Valid) Fold 3 Epoch 1/100: : 8 batches [00:02,  2.82 batches/s, ValLoss=3.0627, Valf1_score=0.0060, Valrmse=3.0329]\n","(Train) Fold 3 Epoch 2/100:   0%|          | 0/24 [00:00<?, ? batches/s]"],"name":"stderr"},{"output_type":"stream","text":["\n"," Val Loss is improved from 9999.0000 to 3.0627! \n"," Val f1 score is improved from -9999.0000 to 0.0060! \n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py:749: UserWarning: Image compression augmentation is most effective with uint8 inputs, float32 is used as input.\n","  UserWarning,\n","/usr/local/lib/python3.6/dist-packages/albumentations/augmentations/functional.py:749: UserWarning: Image compression augmentation is most effective with uint8 inputs, float32 is used as input.\n","  UserWarning,\n","(Train) Fold 3 Epoch 2/100:  46%|████▌     | 11/24 [00:18<00:22,  1.71s/ batches, TrainLoss=3.0143, Trainf1_score=0.0000, Trainrmse=0.0000]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"aPN5iaFFJXKj"},"source":[""],"execution_count":null,"outputs":[]}]}