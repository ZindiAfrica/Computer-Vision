{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOTE:\n",
    "# Run using a TPU-enabled environment\n",
    "# Recommended - Kaggle TPU-enabled kernel (TPU V3-8) - comes with most packages\n",
    "# There would be little differences in result between multiple runs but Leaderboard position is maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from category_encoders import OrdinalEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, Dropout \n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# from kaggle_datasets import KaggleDatasets\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertConfig, TFBertModel\n",
    "from tqdm.notebook import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "import re\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to get AdamW optimizer\n",
    "!pip install -q tf-models-official==2.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from official import nlp\n",
    "import official.nlp.optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Path to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../input/instadeep-enzyme\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(DATA_PATH, \"Train.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(DATA_PATH, \"Test.csv\"))\n",
    "sample_submission = pd.read_csv(os.path.join(DATA_PATH, \"SampleSubmission.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((858777, 4), (253146, 3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include spaces between amino acids\n",
    "df.SEQUENCE = df.SEQUENCE.apply(lambda row: \" \".join(row))\n",
    "df_test.SEQUENCE = df_test.SEQUENCE.apply(lambda row: \" \".join(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column names to lowercase\n",
    "df.rename({\n",
    "    \"SEQUENCE_ID\": \"sequence_id\",\n",
    "    \"SEQUENCE\": \"sequence\",\n",
    "    \"CREATURE\": \"creature\",\n",
    "    \"LABEL\": \"label\",\n",
    "}, axis = 1, inplace = True)\n",
    "\n",
    "df_test.rename({\n",
    "    \"SEQUENCE_ID\": \"sequence_id\",\n",
    "    \"SEQUENCE\": \"sequence\",\n",
    "    \"CREATURE\": \"creature\",\n",
    "}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df = df.drop_duplicates(subset = ['sequence', 'label'], keep = 'first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = OrdinalEncoder(cols = [\"label\"], return_df = False, mapping = \n",
    "                    [{\"col\": \"label\", \"mapping\": {\n",
    "                        \"class0\": 0,\n",
    "                        \"class1\": 1,\n",
    "                        \"class2\": 2,\n",
    "                        \"class3\": 3,\n",
    "                        \"class4\": 4,\n",
    "                        \"class5\": 5,\n",
    "                        \"class6\": 6,\n",
    "                        \"class7\": 7,\n",
    "                        \"class8\": 8,\n",
    "                        \"class9\": 9,\n",
    "                        \"class10\": 10,\n",
    "                        \"class11\": 11,\n",
    "                        \"class12\": 12,\n",
    "                        \"class13\": 13,\n",
    "                        \"class14\": 14,\n",
    "                        \"class15\": 15,\n",
    "                        \"class16\": 16,\n",
    "                        \"class17\": 17,\n",
    "                        \"class18\": 18,\n",
    "                        \"class19\": 19,\n",
    "                    }\n",
    "                     }]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"] = le.fit_transform(df.label)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = df.sequence.values\n",
    "test_sequences = df_test.sequence.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_tpu():\n",
    "    try: # detect TPUs\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() \n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "        \n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError: # otherwise detect GPUs\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "    \n",
    "    return strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "strategy = connect_to_tpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Data access\n",
    "# GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
    "\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 16 * strategy.num_replicas_in_sync # 128\n",
    "MAX_LEN = 384\n",
    "MODEL = \"Rostlab/prot_bert_bfd\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slpit data into train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences, valid_sequences, train_labels, valid_labels = train_test_split(sequences, labels, test_size = 0.2, shuffle = True, stratify = labels, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences.shape, valid_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sequences\n",
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder/Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://www.kaggle.com/xhlulu/jigsaw-tpu-distilbert-with-huggingface-and-keras\n",
    "def fast_encode(texts, tokenizer, chunk_size = 1024, max_len = 384):\n",
    "\n",
    "    tokenizer.enable_truncation(max_length=max_len)\n",
    "    tokenizer.enable_padding()\n",
    "    all_ids = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(texts), chunk_size)):\n",
    "        text_chunk = texts[i:i+chunk_size]#.tolist()\n",
    "        encs = tokenizer.encode_batch(text_chunk, )\n",
    "        all_ids.extend([enc.ids for enc in encs])\n",
    "        del text_chunk\n",
    "        del encs\n",
    "    gc.collect()\n",
    "    \n",
    "    return np.array(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained('.')\n",
    "\n",
    "# Reload tokenizer\n",
    "fast_tokenizer = BertWordPieceTokenizer('./vocab.txt', lowercase=False)\n",
    "fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map rare amino acids \"U,Z,O,B\" to \"X\" according to pretrained model\n",
    "# https://huggingface.co/Rostlab/prot_bert_bfd\n",
    "\n",
    "train_sequences = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in train_sequences]\n",
    "valid_sequences = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in valid_sequences]\n",
    "test_sequences = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in test_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get input ids\n",
    "input_ids_train = fast_encode(train_sequences, fast_tokenizer, max_len = MAX_LEN)\n",
    "# Get attention mask using some heuristics\n",
    "attention_masks_train = (input_ids_train != 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del train_sequences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_valid = fast_encode(valid_sequences, fast_tokenizer, max_len = MAX_LEN)\n",
    "attention_masks_valid = (input_ids_valid != 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del valid_sequences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_test = fast_encode(test_sequences, fast_tokenizer, max_len = MAX_LEN)\n",
    "attention_masks_test = (input_ids_test != 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del test_sequences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(transformer_layer, optimizer, max_len = 384):\n",
    "    \n",
    "    input_word_ids = Input(shape = (max_len,), dtype = tf.int32, name = \"input_ids\")\n",
    "    input_masks = Input(shape = (max_len,),  dtype = tf.int32, name = \"attention_mask\")\n",
    "    \n",
    "    outputs = transformer_layer(input_word_ids, attention_mask = input_masks)\n",
    "    \n",
    "    last_hidden_state = outputs.last_hidden_state   \n",
    "    cls_token = last_hidden_state[:, 0, :]\n",
    "    \n",
    "    out = Dense(20, activation='softmax')(cls_token)   \n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_masks], outputs=out)\n",
    "    \n",
    "    model.compile(optimizer, loss = SparseCategoricalCrossentropy(), metrics=['accuracy'], steps_per_execution = 1)    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertConfig {\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 4096,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 40000,\n",
       "  \"model_type\": \"bert\",\n",
       "  \"num_attention_heads\": 16,\n",
       "  \"num_hidden_layers\": 30,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.2.2\",\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doesn't matter?\n",
    "config.max_position_embeddings = 384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer w/ LR and scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = int(np.ceil(len(input_ids_train) / BATCH_SIZE))\n",
    "num_train_steps = steps_per_epoch * EPOCHS\n",
    "warmup_prop = 0.1\n",
    "warmup_steps = int(warmup_prop * num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High level wrapper that creates an optimizer with learning rate scheduler\n",
    "optimizer = nlp.optimization.create_optimizer(\n",
    "    5e-5, \n",
    "    num_train_steps = num_train_steps, \n",
    "    num_warmup_steps = warmup_steps, \n",
    "    end_lr = 0.0, \n",
    "    optimizer_type = \"adamw\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "official.nlp.optimization.AdamWeightDecay"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "574"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warmup_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect_to_tpu() # For model reruns within same runtime OR restart session to clear TPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data in required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(\n",
    "        (\n",
    "            {\n",
    "        \"input_ids\": input_ids_train,\n",
    "        \"attention_mask\": attention_masks_train\n",
    "            },\n",
    "            train_labels\n",
    "        )\n",
    "    )\n",
    "    .repeat(EPOCHS)\n",
    "    .shuffle(2048, seed = 42)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "print(\"Done\")\n",
    "    \n",
    "valid_data = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(\n",
    "        (\n",
    "            {\n",
    "        \"input_ids\": input_ids_valid,\n",
    "        \"attention_mask\": attention_masks_valid\n",
    "            },\n",
    "            valid_labels\n",
    "        )\n",
    "    )\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()        \n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "test_data = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(\n",
    "        (\n",
    "            {\n",
    "        \"input_ids\": input_ids_test,\n",
    "        \"attention_mask\": attention_masks_test\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model within strategy scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    transformer_layer = TFBertModel.from_pretrained(MODEL)\n",
    "    \n",
    "    model = build_model(transformer_layer, optimizer, max_len = MAX_LEN)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = model.fit(\n",
    "    train_data,\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    validation_data = valid_data,\n",
    "    epochs = EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_data, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sample_submission.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.LABEL = np.argmax(test_predictions, axis = 1)\n",
    "submission.LABEL = 'class' + submission.LABEL.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"final_submission.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
