{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spot the Mask Challenge by #ZindiWeekendz_Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_6px-H7YegN",
        "colab_type": "text"
      },
      "source": [
        "<h1 style=\"text-align:center\"> <b>Image Classification using Transfer Learning in PyTorch <b/><h1>\n",
        "\n",
        "Face masks have become a common public sight in the last few months. The Centers for Disease Control (CDC) recently advised the use of simple cloth face coverings to slow the spread of the virus and help people who may have the virus and do not know it from transmitting it to others. Wearing masks is broadly recognised as critical to reducing community transmission and limiting touching of the face.\n",
        "\n",
        "In a time of concerns about slowing the transmission of COVID-19, increased surveillance combined with AI solutions can improve monitoring and reduce the human effort needed to limit the spread of this disease. The objective of this challenge is to create an image classification machine learning model to accurately predict the likelihood that an image contains a person wearing a face mask, or not. The total dataset contains 1,800+ images of people either wearing masks or not.\n",
        "\n",
        "Our machine learning solution will help policymakers, law enforcement, hospitals, and even commercial businesses ensure that masks are being worn appropriately in public. These solutions can help in the battle to reduce community transmission of COVID-19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G4mQUePX0B9",
        "colab_type": "text"
      },
      "source": [
        "<h2>Import librarys and modules</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8oAf0-ccaFt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import os\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.autograd import Variable\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL \n",
        "from torch import nn\n",
        "import time\n",
        "import random\n",
        "from matplotlib import gridspec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMsmDd7GXSlM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "cd864247-a373-42b4-8eba-6e2f5076ba8b"
      },
      "source": [
        "print(torchvision.__version__)\n",
        "print(torch.__version__)\n",
        "print(PIL.__version__)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5.0\n",
            "1.4.0\n",
            "7.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b1P_PgxWlIf",
        "colab_type": "text"
      },
      "source": [
        "<h2>Set a seed for the random initialization of weights provided by the nn module</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue5h9XweJZf8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "manualSeed = 1\n",
        "random_seed= 42\n",
        "np.random.seed(manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "# if you are suing GPU\n",
        "torch.cuda.manual_seed(manualSeed)\n",
        "torch.cuda.manual_seed_all(manualSeed)\n",
        "\n",
        "\n",
        "torch.backends.cudnn.enabled = False \n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UH7zMLNdVrsh",
        "colab_type": "text"
      },
      "source": [
        "<h2>Download Data</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3In-LJ0aaph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O images.zip --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1vtO5_FB--urbF09XYoxbSAEQW-yqwoi6' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1vtO5_FB--urbF09XYoxbSAEQW-yqwoi6\" && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S22vySq0enat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O train.csv --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1XQ-RfUG1C_6LUZdRUe9nAU77I_AZFH-F' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1XQ-RfUG1C_6LUZdRUe9nAU77I_AZFH-F\" && rm -rf /tmp/cookies.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1CgETJjbu78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip '/content/images.zip'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LoriPHoVX6G",
        "colab_type": "text"
      },
      "source": [
        "<h2>Split Images using Train.csv  provided by Zindi to test and train</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_glTDHDgxmKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('/content/train.csv')\n",
        "\n",
        "train_images_list = train['image'].tolist()\n",
        "\n",
        "images_list = os.listdir('/content/IMG/images')\n",
        "\n",
        "test_images_list = [fn for fn in images_list if fn not in train_images_list]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02T4E37zUy5G",
        "colab_type": "text"
      },
      "source": [
        "<h4>Applying Transforms to the Data,\n",
        "we just resize the images to 256×256 and crop out the center 224×224 in order to be able to use them with the pretrained model. Then the image is transformed into a tensor and normalized by the mean and standard deviation of all images in ImageNet</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0s80BSyxKCDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_transform=transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdyaNpgUtUeb",
        "colab_type": "text"
      },
      "source": [
        "<h2>Create Test images DateFrame</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfFiNmT8Kl0_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub=pd.DataFrame({\n",
        "    'image':test_images_list,\n",
        "     'target':0\n",
        "})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wduheB_UFe3",
        "colab_type": "text"
      },
      "source": [
        "<h2>Transform both train and validation Images</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KzLfcyiB2Rj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_img=[]\n",
        "for i in range(0,ref.shape[0]) :\n",
        "   img_path=\"/content/IMG/images/\"+str(ref.iloc[i,:].image)\n",
        "   image = Image.open(img_path).convert('RGB')\n",
        "   image=image_transform(image)\n",
        "   train_img.append((image,ref.iloc[i,:].target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7LXiQW5TN_P",
        "colab_type": "text"
      },
      "source": [
        "<h2>Download Pre-trained ResNet50 model</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXCfIzB2FCHU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet50 = torchvision.models.resnet50(pretrained=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OKrCvjWTGel",
        "colab_type": "text"
      },
      "source": [
        "<h2> Don't Freeze model parameters</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSB2W4K4F8W6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for param in resnet50.parameters():\n",
        "    param.requires_grad = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emRT9JOKR9zg",
        "colab_type": "text"
      },
      "source": [
        "<h4>Replace the final layer of the ResNet50 model by a small set of Sequential layers. The inputs to the last fully connected layer of ResNet50 is fed to a Linear layer which has 128 outputs, which are then fed into ReLU and Dropout layers. It is then followed by a 128×2 Linear Layer which has 2 outputs corresponding to the 2 classes (mask , without mask).</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdHunrgOGAUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "fc_inputs = resnet50.fc.in_features\n",
        " \n",
        "resnet50.fc = nn.Sequential(\n",
        "    nn.Linear(fc_inputs, 128),\n",
        "    nn.ReLU(inplace=True),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(128,2),\n",
        "    nn.LogSoftmax(dim=1) # For using NLLLoss()\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiZz9eWtGPKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = resnet50.to('cuda:0') \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3pSkfz2RUSY",
        "colab_type": "text"
      },
      "source": [
        "<h2>Define Optimizer and Loss Function</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKBoHY74K0r5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#we have reduced the learning rate to avoid overfitting\n",
        "loss_func = nn.NLLLoss()\n",
        "optimizer =torch.optim.Adam(model.parameters(), lr=0.000099)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiWqVWrdROz4",
        "colab_type": "text"
      },
      "source": [
        "<h2>Creating data indices for training and validation splits</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOH3aG7v4taK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "batch_size = 32\n",
        "validation_split = .0\n",
        "shuffle_dataset = True\n",
        "random_seed= 42\n",
        "dataset_size = len(train_img)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset :\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_img, batch_size=batch_size, \n",
        "                                           sampler=train_sampler)\n",
        "validation_loader = torch.utils.data.DataLoader(train_img, batch_size=batch_size,\n",
        "                                                sampler=valid_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8L8QD1VMky7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMUInv_JtK1G",
        "colab_type": "text"
      },
      "source": [
        "<H2>Train<h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwNHVZHlHC2o",
        "colab_type": "code",
        "outputId": "e64f26dd-bf91-4459-c189-b9914ed59e29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs=5                           \n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    epoch_start = time.time()\n",
        "    print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "    # Set to training mode\n",
        "    model.train()\n",
        "     \n",
        "    # Loss and Accuracy within the epoch\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "     \n",
        "    valid_loss = 0.0\n",
        "    valid_acc = 0.0\n",
        " \n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        " \n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "         \n",
        "        # Clean existing gradients\n",
        "        optimizer.zero_grad()\n",
        "         \n",
        "        # Forward pass - compute outputs on input data using the model\n",
        "        outputs = model(inputs)\n",
        "         \n",
        "        # Compute loss\n",
        "        #print(outputs)\n",
        "        loss = loss_func(outputs, labels)\n",
        "         \n",
        "        # Backpropagate the gradients\n",
        "        loss.backward()\n",
        "         \n",
        "        # Update the parameters\n",
        "        optimizer.step()\n",
        "         \n",
        "        # Compute the total loss for the batch and add it to train_loss\n",
        "        train_loss += loss.item() * inputs.size(0)\n",
        "         \n",
        "        # Compute the accuracy\n",
        "        ret, predictions = torch.max(outputs.data, 1)\n",
        "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "         \n",
        "        # Convert correct_counts to float and then compute the mean\n",
        "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "         \n",
        "        # Compute total accuracy in the whole batch and add to train_acc\n",
        "        train_acc += acc.item() * inputs.size(0)\n",
        "         \n",
        "        print(\"Batch number: {:03d}, Training: Loss: {:.8f}, Accuracy: {:.8f}\".format(i, loss.item(), acc.item()))\n",
        "    "
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/5\n",
            "Batch number: 000, Training: Loss: 0.73168004, Accuracy: 0.43750000\n",
            "Batch number: 001, Training: Loss: 0.64320904, Accuracy: 0.68750000\n",
            "Batch number: 002, Training: Loss: 0.65000010, Accuracy: 0.62500000\n",
            "Batch number: 003, Training: Loss: 0.57134289, Accuracy: 0.75000000\n",
            "Batch number: 004, Training: Loss: 0.62197405, Accuracy: 0.75000000\n",
            "Batch number: 005, Training: Loss: 0.52861047, Accuracy: 0.90625000\n",
            "Batch number: 006, Training: Loss: 0.46683332, Accuracy: 0.87500000\n",
            "Batch number: 007, Training: Loss: 0.43747646, Accuracy: 0.81250000\n",
            "Batch number: 008, Training: Loss: 0.46984932, Accuracy: 0.84375000\n",
            "Batch number: 009, Training: Loss: 0.39817497, Accuracy: 0.81250000\n",
            "Batch number: 010, Training: Loss: 0.30969232, Accuracy: 0.90625000\n",
            "Batch number: 011, Training: Loss: 0.27941290, Accuracy: 0.90625000\n",
            "Batch number: 012, Training: Loss: 0.29774529, Accuracy: 0.90625000\n",
            "Batch number: 013, Training: Loss: 0.23097536, Accuracy: 0.96875000\n",
            "Batch number: 014, Training: Loss: 0.30134356, Accuracy: 0.81250000\n",
            "Batch number: 015, Training: Loss: 0.14795981, Accuracy: 0.96875000\n",
            "Batch number: 016, Training: Loss: 0.27974507, Accuracy: 0.90625000\n",
            "Batch number: 017, Training: Loss: 0.07695771, Accuracy: 1.00000000\n",
            "Batch number: 018, Training: Loss: 0.10158400, Accuracy: 1.00000000\n",
            "Batch number: 019, Training: Loss: 0.17246424, Accuracy: 0.90625000\n",
            "Batch number: 020, Training: Loss: 0.06380773, Accuracy: 1.00000000\n",
            "Batch number: 021, Training: Loss: 0.16364291, Accuracy: 0.90625000\n",
            "Batch number: 022, Training: Loss: 0.07416138, Accuracy: 1.00000000\n",
            "Batch number: 023, Training: Loss: 0.13266015, Accuracy: 0.93750000\n",
            "Batch number: 024, Training: Loss: 0.06595623, Accuracy: 1.00000000\n",
            "Batch number: 025, Training: Loss: 0.06221587, Accuracy: 1.00000000\n",
            "Batch number: 026, Training: Loss: 0.18427601, Accuracy: 0.93750000\n",
            "Batch number: 027, Training: Loss: 0.04502761, Accuracy: 1.00000000\n",
            "Batch number: 028, Training: Loss: 0.02347079, Accuracy: 1.00000000\n",
            "Batch number: 029, Training: Loss: 0.09059100, Accuracy: 0.96875000\n",
            "Batch number: 030, Training: Loss: 0.06364749, Accuracy: 1.00000000\n",
            "Batch number: 031, Training: Loss: 0.04929141, Accuracy: 1.00000000\n",
            "Batch number: 032, Training: Loss: 0.12661082, Accuracy: 0.96875000\n",
            "Batch number: 033, Training: Loss: 0.14188522, Accuracy: 0.96875000\n",
            "Batch number: 034, Training: Loss: 0.07282211, Accuracy: 0.96875000\n",
            "Batch number: 035, Training: Loss: 0.02110376, Accuracy: 1.00000000\n",
            "Batch number: 036, Training: Loss: 0.11036955, Accuracy: 0.96875000\n",
            "Batch number: 037, Training: Loss: 0.04953522, Accuracy: 1.00000000\n",
            "Batch number: 038, Training: Loss: 0.05425328, Accuracy: 0.96875000\n",
            "Batch number: 039, Training: Loss: 0.02956877, Accuracy: 1.00000000\n",
            "Batch number: 040, Training: Loss: 0.04625175, Accuracy: 1.00000000\n",
            "************************************eval**************************************\n",
            "shape: 9\n",
            "test error : 0.026509386160922892\n",
            "Epoch: 2/5\n",
            "Batch number: 000, Training: Loss: 0.06850842, Accuracy: 0.96875000\n",
            "Batch number: 001, Training: Loss: 0.07573829, Accuracy: 0.96875000\n",
            "Batch number: 002, Training: Loss: 0.00996589, Accuracy: 1.00000000\n",
            "Batch number: 003, Training: Loss: 0.02240239, Accuracy: 1.00000000\n",
            "Batch number: 004, Training: Loss: 0.00895225, Accuracy: 1.00000000\n",
            "Batch number: 005, Training: Loss: 0.02742043, Accuracy: 1.00000000\n",
            "Batch number: 006, Training: Loss: 0.00904430, Accuracy: 1.00000000\n",
            "Batch number: 007, Training: Loss: 0.01448383, Accuracy: 1.00000000\n",
            "Batch number: 008, Training: Loss: 0.03430217, Accuracy: 1.00000000\n",
            "Batch number: 009, Training: Loss: 0.01513295, Accuracy: 1.00000000\n",
            "Batch number: 010, Training: Loss: 0.02161916, Accuracy: 1.00000000\n",
            "Batch number: 011, Training: Loss: 0.00706070, Accuracy: 1.00000000\n",
            "Batch number: 012, Training: Loss: 0.00905243, Accuracy: 1.00000000\n",
            "Batch number: 013, Training: Loss: 0.25140157, Accuracy: 0.93750000\n",
            "Batch number: 014, Training: Loss: 0.11869325, Accuracy: 0.93750000\n",
            "Batch number: 015, Training: Loss: 0.03819513, Accuracy: 0.96875000\n",
            "Batch number: 016, Training: Loss: 0.06692369, Accuracy: 0.96875000\n",
            "Batch number: 017, Training: Loss: 0.01638540, Accuracy: 1.00000000\n",
            "Batch number: 018, Training: Loss: 0.02312296, Accuracy: 1.00000000\n",
            "Batch number: 019, Training: Loss: 0.05509066, Accuracy: 0.96875000\n",
            "Batch number: 020, Training: Loss: 0.02298915, Accuracy: 1.00000000\n",
            "Batch number: 021, Training: Loss: 0.07128409, Accuracy: 0.96875000\n",
            "Batch number: 022, Training: Loss: 0.00701545, Accuracy: 1.00000000\n",
            "Batch number: 023, Training: Loss: 0.01165799, Accuracy: 1.00000000\n",
            "Batch number: 024, Training: Loss: 0.02236221, Accuracy: 1.00000000\n",
            "Batch number: 025, Training: Loss: 0.03413976, Accuracy: 1.00000000\n",
            "Batch number: 026, Training: Loss: 0.05461247, Accuracy: 0.96875000\n",
            "Batch number: 027, Training: Loss: 0.04354116, Accuracy: 1.00000000\n",
            "Batch number: 028, Training: Loss: 0.18627523, Accuracy: 0.93750000\n",
            "Batch number: 029, Training: Loss: 0.01367302, Accuracy: 1.00000000\n",
            "Batch number: 030, Training: Loss: 0.00777497, Accuracy: 1.00000000\n",
            "Batch number: 031, Training: Loss: 0.08973461, Accuracy: 0.96875000\n",
            "Batch number: 032, Training: Loss: 0.02316102, Accuracy: 1.00000000\n",
            "Batch number: 033, Training: Loss: 0.01419867, Accuracy: 1.00000000\n",
            "Batch number: 034, Training: Loss: 0.01120914, Accuracy: 1.00000000\n",
            "Batch number: 035, Training: Loss: 0.02369521, Accuracy: 1.00000000\n",
            "Batch number: 036, Training: Loss: 0.01984859, Accuracy: 1.00000000\n",
            "Batch number: 037, Training: Loss: 0.00897614, Accuracy: 1.00000000\n",
            "Batch number: 038, Training: Loss: 0.00987574, Accuracy: 1.00000000\n",
            "Batch number: 039, Training: Loss: 0.05981832, Accuracy: 0.93750000\n",
            "Batch number: 040, Training: Loss: 0.04334949, Accuracy: 0.96428573\n",
            "************************************eval**************************************\n",
            "shape: 7\n",
            "test error : 0.02370270795871038\n",
            "Epoch: 3/5\n",
            "Batch number: 000, Training: Loss: 0.03393913, Accuracy: 1.00000000\n",
            "Batch number: 001, Training: Loss: 0.04763194, Accuracy: 0.96875000\n",
            "Batch number: 002, Training: Loss: 0.21868688, Accuracy: 0.93750000\n",
            "Batch number: 003, Training: Loss: 0.00582657, Accuracy: 1.00000000\n",
            "Batch number: 004, Training: Loss: 0.00965899, Accuracy: 1.00000000\n",
            "Batch number: 005, Training: Loss: 0.00498629, Accuracy: 1.00000000\n",
            "Batch number: 006, Training: Loss: 0.01171814, Accuracy: 1.00000000\n",
            "Batch number: 007, Training: Loss: 0.02732695, Accuracy: 0.96875000\n",
            "Batch number: 008, Training: Loss: 0.00973634, Accuracy: 1.00000000\n",
            "Batch number: 009, Training: Loss: 0.01204853, Accuracy: 1.00000000\n",
            "Batch number: 010, Training: Loss: 0.01195552, Accuracy: 1.00000000\n",
            "Batch number: 011, Training: Loss: 0.01999247, Accuracy: 1.00000000\n",
            "Batch number: 012, Training: Loss: 0.00697360, Accuracy: 1.00000000\n",
            "Batch number: 013, Training: Loss: 0.01093524, Accuracy: 1.00000000\n",
            "Batch number: 014, Training: Loss: 0.09086918, Accuracy: 0.96875000\n",
            "Batch number: 015, Training: Loss: 0.00839328, Accuracy: 1.00000000\n",
            "Batch number: 016, Training: Loss: 0.00852025, Accuracy: 1.00000000\n",
            "Batch number: 017, Training: Loss: 0.01341246, Accuracy: 1.00000000\n",
            "Batch number: 018, Training: Loss: 0.00814619, Accuracy: 1.00000000\n",
            "Batch number: 019, Training: Loss: 0.00711526, Accuracy: 1.00000000\n",
            "Batch number: 020, Training: Loss: 0.00379765, Accuracy: 1.00000000\n",
            "Batch number: 021, Training: Loss: 0.02281969, Accuracy: 1.00000000\n",
            "Batch number: 022, Training: Loss: 0.00202069, Accuracy: 1.00000000\n",
            "Batch number: 023, Training: Loss: 0.00603458, Accuracy: 1.00000000\n",
            "Batch number: 024, Training: Loss: 0.00478073, Accuracy: 1.00000000\n",
            "Batch number: 025, Training: Loss: 0.01257742, Accuracy: 1.00000000\n",
            "Batch number: 026, Training: Loss: 0.00268158, Accuracy: 1.00000000\n",
            "Batch number: 027, Training: Loss: 0.00440767, Accuracy: 1.00000000\n",
            "Batch number: 028, Training: Loss: 0.00664226, Accuracy: 1.00000000\n",
            "Batch number: 029, Training: Loss: 0.00933707, Accuracy: 1.00000000\n",
            "Batch number: 030, Training: Loss: 0.03970980, Accuracy: 0.96875000\n",
            "Batch number: 031, Training: Loss: 0.00728894, Accuracy: 1.00000000\n",
            "Batch number: 032, Training: Loss: 0.02228521, Accuracy: 1.00000000\n",
            "Batch number: 033, Training: Loss: 0.01485514, Accuracy: 1.00000000\n",
            "Batch number: 034, Training: Loss: 0.01493356, Accuracy: 1.00000000\n",
            "Batch number: 035, Training: Loss: 0.00389309, Accuracy: 1.00000000\n",
            "Batch number: 036, Training: Loss: 0.00666954, Accuracy: 1.00000000\n",
            "Batch number: 037, Training: Loss: 0.00913894, Accuracy: 1.00000000\n",
            "Batch number: 038, Training: Loss: 0.02022195, Accuracy: 1.00000000\n",
            "Batch number: 039, Training: Loss: 0.00655237, Accuracy: 1.00000000\n",
            "Batch number: 040, Training: Loss: 0.01505656, Accuracy: 1.00000000\n",
            "************************************eval**************************************\n",
            "shape: 9\n",
            "test error : 0.02150127265238228\n",
            "Epoch: 4/5\n",
            "Batch number: 000, Training: Loss: 0.00207184, Accuracy: 1.00000000\n",
            "Batch number: 001, Training: Loss: 0.01229604, Accuracy: 1.00000000\n",
            "Batch number: 002, Training: Loss: 0.00268511, Accuracy: 1.00000000\n",
            "Batch number: 003, Training: Loss: 0.00373584, Accuracy: 1.00000000\n",
            "Batch number: 004, Training: Loss: 0.00275776, Accuracy: 1.00000000\n",
            "Batch number: 005, Training: Loss: 0.00535326, Accuracy: 1.00000000\n",
            "Batch number: 006, Training: Loss: 0.00426840, Accuracy: 1.00000000\n",
            "Batch number: 007, Training: Loss: 0.00189088, Accuracy: 1.00000000\n",
            "Batch number: 008, Training: Loss: 0.10235173, Accuracy: 0.96875000\n",
            "Batch number: 009, Training: Loss: 0.00716513, Accuracy: 1.00000000\n",
            "Batch number: 010, Training: Loss: 0.00205149, Accuracy: 1.00000000\n",
            "Batch number: 011, Training: Loss: 0.00190628, Accuracy: 1.00000000\n",
            "Batch number: 012, Training: Loss: 0.00229764, Accuracy: 1.00000000\n",
            "Batch number: 013, Training: Loss: 0.00185361, Accuracy: 1.00000000\n",
            "Batch number: 014, Training: Loss: 0.00343170, Accuracy: 1.00000000\n",
            "Batch number: 015, Training: Loss: 0.00197599, Accuracy: 1.00000000\n",
            "Batch number: 016, Training: Loss: 0.00180166, Accuracy: 1.00000000\n",
            "Batch number: 017, Training: Loss: 0.01367618, Accuracy: 1.00000000\n",
            "Batch number: 018, Training: Loss: 0.00273555, Accuracy: 1.00000000\n",
            "Batch number: 019, Training: Loss: 0.00413901, Accuracy: 1.00000000\n",
            "Batch number: 020, Training: Loss: 0.00344237, Accuracy: 1.00000000\n",
            "Batch number: 021, Training: Loss: 0.00204055, Accuracy: 1.00000000\n",
            "Batch number: 022, Training: Loss: 0.00161564, Accuracy: 1.00000000\n",
            "Batch number: 023, Training: Loss: 0.00154173, Accuracy: 1.00000000\n",
            "Batch number: 024, Training: Loss: 0.00197979, Accuracy: 1.00000000\n",
            "Batch number: 025, Training: Loss: 0.00115684, Accuracy: 1.00000000\n",
            "Batch number: 026, Training: Loss: 0.01470910, Accuracy: 1.00000000\n",
            "Batch number: 027, Training: Loss: 0.00209863, Accuracy: 1.00000000\n",
            "Batch number: 028, Training: Loss: 0.00184521, Accuracy: 1.00000000\n",
            "Batch number: 029, Training: Loss: 0.00254835, Accuracy: 1.00000000\n",
            "Batch number: 030, Training: Loss: 0.00105017, Accuracy: 1.00000000\n",
            "Batch number: 031, Training: Loss: 0.00285306, Accuracy: 1.00000000\n",
            "Batch number: 032, Training: Loss: 0.00391828, Accuracy: 1.00000000\n",
            "Batch number: 033, Training: Loss: 0.01264434, Accuracy: 1.00000000\n",
            "Batch number: 034, Training: Loss: 0.00358269, Accuracy: 1.00000000\n",
            "Batch number: 035, Training: Loss: 0.00268983, Accuracy: 1.00000000\n",
            "Batch number: 036, Training: Loss: 0.00205059, Accuracy: 1.00000000\n",
            "Batch number: 037, Training: Loss: 0.00111558, Accuracy: 1.00000000\n",
            "Batch number: 038, Training: Loss: 0.00138549, Accuracy: 1.00000000\n",
            "Batch number: 039, Training: Loss: 0.00271299, Accuracy: 1.00000000\n",
            "Batch number: 040, Training: Loss: 0.01124305, Accuracy: 1.00000000\n",
            "************************************eval**************************************\n",
            "shape: 6\n",
            "test error : 0.014429636154227649\n",
            "Epoch: 5/5\n",
            "Batch number: 000, Training: Loss: 0.00182034, Accuracy: 1.00000000\n",
            "Batch number: 001, Training: Loss: 0.00141840, Accuracy: 1.00000000\n",
            "Batch number: 002, Training: Loss: 0.00124522, Accuracy: 1.00000000\n",
            "Batch number: 003, Training: Loss: 0.00129405, Accuracy: 1.00000000\n",
            "Batch number: 004, Training: Loss: 0.00388184, Accuracy: 1.00000000\n",
            "Batch number: 005, Training: Loss: 0.00048695, Accuracy: 1.00000000\n",
            "Batch number: 006, Training: Loss: 0.00146687, Accuracy: 1.00000000\n",
            "Batch number: 007, Training: Loss: 0.00123543, Accuracy: 1.00000000\n",
            "Batch number: 008, Training: Loss: 0.00142562, Accuracy: 1.00000000\n",
            "Batch number: 009, Training: Loss: 0.00174963, Accuracy: 1.00000000\n",
            "Batch number: 010, Training: Loss: 0.00345122, Accuracy: 1.00000000\n",
            "Batch number: 011, Training: Loss: 0.00124739, Accuracy: 1.00000000\n",
            "Batch number: 012, Training: Loss: 0.00398643, Accuracy: 1.00000000\n",
            "Batch number: 013, Training: Loss: 0.00100248, Accuracy: 1.00000000\n",
            "Batch number: 014, Training: Loss: 0.00256528, Accuracy: 1.00000000\n",
            "Batch number: 015, Training: Loss: 0.00182895, Accuracy: 1.00000000\n",
            "Batch number: 016, Training: Loss: 0.00167540, Accuracy: 1.00000000\n",
            "Batch number: 017, Training: Loss: 0.00153704, Accuracy: 1.00000000\n",
            "Batch number: 018, Training: Loss: 0.00151651, Accuracy: 1.00000000\n",
            "Batch number: 019, Training: Loss: 0.00173813, Accuracy: 1.00000000\n",
            "Batch number: 020, Training: Loss: 0.00165532, Accuracy: 1.00000000\n",
            "Batch number: 021, Training: Loss: 0.00076499, Accuracy: 1.00000000\n",
            "Batch number: 022, Training: Loss: 0.00151483, Accuracy: 1.00000000\n",
            "Batch number: 023, Training: Loss: 0.00205093, Accuracy: 1.00000000\n",
            "Batch number: 024, Training: Loss: 0.00094158, Accuracy: 1.00000000\n",
            "Batch number: 025, Training: Loss: 0.00064114, Accuracy: 1.00000000\n",
            "Batch number: 026, Training: Loss: 0.00048158, Accuracy: 1.00000000\n",
            "Batch number: 027, Training: Loss: 0.00159898, Accuracy: 1.00000000\n",
            "Batch number: 028, Training: Loss: 0.00188274, Accuracy: 1.00000000\n",
            "Batch number: 029, Training: Loss: 0.00069462, Accuracy: 1.00000000\n",
            "Batch number: 030, Training: Loss: 0.00079446, Accuracy: 1.00000000\n",
            "Batch number: 031, Training: Loss: 0.00182399, Accuracy: 1.00000000\n",
            "Batch number: 032, Training: Loss: 0.00092866, Accuracy: 1.00000000\n",
            "Batch number: 033, Training: Loss: 0.00550598, Accuracy: 1.00000000\n",
            "Batch number: 034, Training: Loss: 0.00059937, Accuracy: 1.00000000\n",
            "Batch number: 035, Training: Loss: 0.00083408, Accuracy: 1.00000000\n",
            "Batch number: 036, Training: Loss: 0.00040286, Accuracy: 1.00000000\n",
            "Batch number: 037, Training: Loss: 0.00070605, Accuracy: 1.00000000\n",
            "Batch number: 038, Training: Loss: 0.00169741, Accuracy: 1.00000000\n",
            "Batch number: 039, Training: Loss: 0.00088479, Accuracy: 1.00000000\n",
            "Batch number: 040, Training: Loss: 0.00283624, Accuracy: 1.00000000\n",
            "************************************eval**************************************\n",
            "shape: 5\n",
            "test error : 0.014457869193020398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfjzJtGzRCel",
        "colab_type": "text"
      },
      "source": [
        "<h2> Create checkpoint for our Model </h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm2aBCYh808s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "70b8e11a-4049-4245-d086-389889574396"
      },
      "source": [
        "\n",
        "torch.save(model.state_dict(),'model.pt')\n",
        "model.load_state_dict(torch.load('/content/model.pt'))\n",
        "m=model.eval()\n",
        "m"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Sequential(\n",
              "    (0): Linear(in_features=2048, out_features=128, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=128, out_features=2, bias=True)\n",
              "    (4): LogSoftmax()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNHDkr5XtERk",
        "colab_type": "text"
      },
      "source": [
        "<H2>Validation<h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqt9ehJXNMcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "history=[]\n",
        "with torch.no_grad():\n",
        " \n",
        "    # Set to evaluation mode\n",
        "    m.eval()\n",
        " \n",
        "    # Validation loop\n",
        "    for j, (inputs, labels) in enumerate(validation_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        " \n",
        "        # Forward pass - compute outputs on input data using the model\n",
        "        outputs = m(inputs)\n",
        " \n",
        "        # Compute loss\n",
        "        loss = loss_func(outputs, labels)\n",
        " \n",
        "        # Compute the total loss for the batch and add it to valid_loss\n",
        "        valid_loss += loss.item() * inputs.size(0)\n",
        " \n",
        "        # Calculate validation accuracy\n",
        "        ret, predictions = torch.max(outputs.data, 1)\n",
        "        correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        " \n",
        "        # Convert correct_counts to float and then compute the mean\n",
        "        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        " \n",
        "        # Compute total accuracy in the whole batch and add to valid_acc\n",
        "        valid_acc += acc.item() * inputs.size(0)\n",
        " \n",
        "        print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "     \n",
        "# Find average training loss and training accuracy\n",
        "avg_train_loss = train_loss/len(train_indices)\n",
        "avg_train_acc = train_acc/float(len(train_indices))\n",
        " \n",
        "# Find average training loss and training accuracy\n",
        "avg_valid_loss = valid_loss/len(val_indices)\n",
        "avg_valid_acc = valid_acc/float(len(val_indices))\n",
        " \n",
        "history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])\n",
        "         \n",
        "epoch_end = time.time()\n",
        " \n",
        "print(\"Epoch : {:03d}, Training: Loss: {:.8f}, Accuracy: {:.8f}%, \\n\\t\\tValidation : Loss : {:.8f}, Accuracy: {:.8f}%, Time: {:.8f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFQ3BlkPN0LL",
        "colab_type": "text"
      },
      "source": [
        "<h2>Test<h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5cNDzVUM3ck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "    test_img=[]\n",
        "    for img in sub['image'] :\n",
        "          img_path=\"/content/IMG/images/\"+str(img)\n",
        "          test_image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "          test_image_tensor = image_transform(test_image)\n",
        "          if torch.cuda.is_available():\n",
        "                test_image_tensor = test_image_tensor.view(1, 3, 224, 224).cuda()\n",
        "          else:\n",
        "                test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n",
        "\n",
        "          #test_image_tensor = test_image_tensor.view(1, 3, 224, 224)\n",
        "          with torch.no_grad():\n",
        "            m.eval()\n",
        "            out=m(test_image_tensor)\n",
        "            ps = torch.exp(out)\n",
        "            topk, topclass = ps.topk(1, dim=1,)\n",
        "            sub.loc[sub['image']==img,'target']=float(ps[0][1])\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ewyyWR1gLyV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0HIWwL4QGB9",
        "colab_type": "text"
      },
      "source": [
        "<h2>Dispaly images<h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaFGwp2iREvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "show=sub\n",
        "nrow = show.shape[0]\n",
        "ncol = 1\n",
        "\n",
        "fig = plt.figure(figsize=(4, 10)) \n",
        "\n",
        "gs = gridspec.GridSpec(nrow, ncol,\n",
        "         wspace=0.0, hspace=0.0, top=0.95, bottom=0.05, left=0.17, right=0.845) \n",
        "\n",
        "\n",
        "for num, x in enumerate(show.image):\n",
        "        img_path=\"/content/adem/images/\"+str(x)\n",
        "        im= PIL.Image.open(img_path).convert('RGB')\n",
        "        ax= plt.subplot(gs[num,0])\n",
        "        fig = plt.figure()\n",
        "        fig.set_size_inches(150,150)\n",
        "        ax.imshow(im)\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o0HTXz_QRII",
        "colab_type": "text"
      },
      "source": [
        "<h2>Create CSV File<h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrHzu7AnTY_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can round prediction to improve score to 0.009 and , without them score=0.014\n",
        "#sub.target=np.where(sub.target<0.18,0,sub.target)\n",
        "#sub.target=np.where(sub.target>0.6,1,sub.target)\n",
        "sub.to_csv('FinalSub.csv',index=False)\n",
        "sub"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}