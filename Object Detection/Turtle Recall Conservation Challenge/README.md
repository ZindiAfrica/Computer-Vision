
# Competition Summary

## Description

Being able to distinguish between individuals of the same species is a critical tool for modern conservation.

Sea turtle conservation efforts aim to track individual turtles to help reveal patterns of movement and residency.

Sea turtles can be identified using their facial scales, which are as unique as a human fingerprint.

Traditionally, individual recognition has been achieved manually through the attachment of tags on the flippers of found individuals.

Moreover, tags are expensive and through the long duration of sea turtle life cycles they can deteriorate and require a replacement.

The aim of this competition is to build a machine learning model to identify individual sea turtles. 


## Competition Rules

Participation in this competition could be as an individual or in a team of up to four people.

Prizes are transferred only to the individual players or to the team leader.

Code was not shared privately outside of a team. Any code shared, was made available to all competition participants through the platform. (i.e. on the discussion boards).


## Datasets and packages

The data was collected through the Watamu Turtle Watch and Local Ocean Conservation.

Input data consists of a set of labelled JPG images of turtle faces from three different perspectives: top, left, right.

There are 100 distinct turtle identities present in the training set. 

An additional set of labelled images has also been provided.


## Submissions and winning

The top 3 solution placed on the final leaderboard were required to submit their winning solution code to us for verification, and thereby agreed to assign all worldwide rights of copyright in and to such winning solution to Zindi.


## Reproducibility

The full documentation was retrieved. This includes:
- All data used

- Output data and where they are stored

- Explanation of features used

- The solution must include the original data provided by Zindi and validated external data (no processed data)

- All editing of data must be done in a notebook (i.e. not manually in Excel)


## Data standards:

- The most recent versions of packages were used.

- Submitted code run on the original train, test, and other datasets provided.


## Evaluation:

The evaluation metric for this challenge is Log Loss.

## Prizes

1st Place: $5 000 USD
2nd Place: $3 000 USD
3rd Place: $1 000 USD

Generalisability prize: $1 000 USD

Researchers from DeepMind examined the top 10 entries on the leaderboard, based on a review of the corresponding code, taking into account the approach and algorithm used, to determine how likely it would be to generalise beyond the challenge dataset


## Benefits

The Solutions will increase our ability to identify and understand them can enhance our ecological understanding.

